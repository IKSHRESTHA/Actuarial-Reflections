[
  {
    "objectID": "posts/understanding risk.html",
    "href": "posts/understanding risk.html",
    "title": "Understanding Risk",
    "section": "",
    "text": "Risk is an unavoidable part of life, woven into every decision we make—whether we are running a business, planning a project, or simply crossing the street. In its simplest form, risk is the possibility that something unexpected or undesirable will happen, leading to negative consequences. But in the world of actuarial science and insurance, risk is much more than a vague sense of uncertainty; it is a concept that can be measured, analyzed, and, to some extent, managed.\n\n\nTo truly understand risk, it helps to break it down into its core elements. At its heart, risk is defined by two main factors: probability and impact. Probability is the likelihood that a particular event will occur, while impact is the severity of the consequences if it does. For example, the risk of rain on a given day depends on the weather forecast (probability), but the impact of that rain will be very different if you are planning a picnic versus if you are a farmer hoping for crops to grow.\nIn business and insurance, risk is often described as the combination of the chance that something will go wrong and the cost or harm that would result. Managing risk means understanding both how often something might go wrong and how serious it would be if it does. This dual perspective allows individuals and organizations to make informed decisions about which risks to accept, which to avoid, and which to transfer to others (such as through insurance).\n\n\n\nRisk is everywhere. When you drive a car, you face the risk of an accident. When you invest money, you risk losing it if the market falls. Even something as simple as eating at a new restaurant carries the risk of food poisoning. Some risks are so minor that we barely notice them, while others can have life-changing consequences.\nConsider a small business owner. She faces the risk that her products might not sell, that a fire could damage her shop, or that an employee might get injured at work. Each of these risks has a different probability and impact, and each requires a different approach to management.\n\n\n\nWhen thinking about risk, always ask two questions:\n\nHow likely is it to happen? (Probability)\nHow bad would it be if it happens? (Impact)\n\nFor example, the risk of a minor power outage in a city might be fairly high (it happens often), but the impact is usually small (a brief inconvenience). In contrast, the risk of a major earthquake is low (it rarely happens), but the impact can be catastrophic.\n\n\n\nRisks can be grouped in many ways, but one of the most useful is by considering both their likelihood and their impact. This approach helps prioritize which risks deserve the most attention. Let’s explore four common types of risk, with detailed examples and management strategies for each.\n\n\nMinor risks are those that are unlikely to happen and, even if they do, would not cause much harm. These are the everyday annoyances and small setbacks that are part of life. For example, running out of printer paper at the office is a minor risk. It might slow you down for a few minutes, but it is easily fixed and rarely has serious consequences.\nIn most cases, the best way to manage minor risks is simply to accept them. It is not worth spending a lot of time or money trying to prevent every small inconvenience. However, it is wise to monitor these risks to make sure they do not become more serious over time. For instance, if running out of printer paper starts happening every week, it might be a sign that you need a better system for ordering supplies.\n\n\n\nSome risks are unlikely to occur, but if they do, the consequences can be severe. These are the risks that keep business owners and families awake at night. Natural disasters like earthquakes, floods, or fires fall into this category. The probability of a major earthquake in a given year is low, but the impact can be devastating—destroying homes, businesses, and lives.\nManaging rare but serious risks requires careful planning. One common strategy is to develop contingency plans—detailed steps to follow if the worst happens. For example, a family might have an emergency kit and a plan for where to meet if their home is damaged in an earthquake. Businesses often buy insurance to transfer some of the financial risk to an insurer. Regularly reviewing and updating these plans is essential, as circumstances and risks can change over time.\n\n\n\nThese are risks that happen often, but their effects are minor. For example, a company might experience frequent minor IT glitches that slow down work but do not cause major losses. In a household, this could be the risk of small kitchen accidents, like spilling water or burning toast.\nThe best way to manage frequent, low-impact risks is to reduce how often they happen. This might mean improving processes, providing better training, or maintaining equipment more regularly. Keeping records of how often these risks occur can help spot trends and identify areas for improvement. For example, if a company notices that IT glitches are becoming more common, it might be time to upgrade its systems or provide additional staff training.\n\n\n\nCritical risks are both likely to happen and would have major consequences. These are the risks that demand immediate attention and strong controls. For example, a hospital faces the critical risk of a power failure during surgery. The probability may not be high, but the impact is so severe that it cannot be ignored. Another example is the risk of a cyberattack on a company that stores sensitive customer data. Such an event is both increasingly likely and potentially disastrous.\nManaging critical risks requires a proactive approach. Organizations must act immediately to address these risks, implementing strong controls and safeguards. This might include installing backup generators in a hospital, setting up firewalls and security protocols for IT systems, or developing and testing detailed response plans. Regular monitoring and review are essential to ensure that controls remain effective as threats evolve.\n\n\n\n\nOne of the most important questions in actuarial science and insurance is whether a risk is insurable. Not all risks can be covered by insurance, and understanding the difference is crucial for both individuals and businesses.\n\n\nFor a risk to be insurable, it generally needs to meet several criteria:\n\nThe risk must be definable and measurable. Insurers need to know what event they are covering and be able to estimate the probability and potential loss.\nThe loss must be accidental and unintentional. Insurance is designed to cover unforeseen events, not losses that are certain or deliberate.\nThe loss must be significant enough to cause financial hardship, but not so catastrophic that it would bankrupt the insurer.\nThere must be a large number of similar exposure units. This allows insurers to pool risks and use the law of large numbers to predict losses.\nThe probability of loss must be calculable. Insurers rely on data and statistics to set premiums and reserves.\nThe premium must be affordable. If the cost of insurance is too high, few people will buy it.\n\n\n\n\nMost common insurance policies cover risks that meet these criteria. For example:\n\nFire insurance covers the risk of accidental fire damaging a home or business. Fires are relatively rare, but the losses can be significant, and insurers have enough data to estimate the probability and cost.\nHealth insurance covers the risk of illness or injury. While everyone gets sick at some point, the timing and severity are unpredictable, and insurers can pool risks across many policyholders.\nAuto insurance covers the risk of car accidents. Again, accidents are accidental, measurable, and there is enough data to set premiums.\n\n\n\n\nSome risks cannot be insured, either because they are too certain, too catastrophic, or impossible to measure. For example:\n\nWear and tear on a car or machine is not insurable, because it is certain to happen over time.\nLosses from illegal activities or intentional acts are not covered, because insurance is not meant to reward bad behavior.\nWar and nuclear disasters are often excluded from insurance policies, because the potential losses are so large and unpredictable that no insurer could cover them.\nSpeculative risks, such as gambling losses or investment losses, are generally not insurable, because they involve the chance of gain as well as loss, and are not accidental.\n\n\n\n\n\nLet’s look at some practical examples to make these concepts clearer:\nScenario 1: Insurable Risk\nSuppose you own a small bakery. You are worried about the risk of a fire destroying your shop. This is an insurable risk: it is accidental, measurable, and there is enough data for insurers to set a fair premium. You can buy fire insurance to protect your business.\nScenario 2: Uninsurable Risk\nNow imagine you are concerned that your bakery might not be popular and could fail because customers do not like your bread. This is a business risk, but it is not insurable. The risk of business failure due to lack of demand is too uncertain, and it is influenced by your own actions and market conditions. No insurer will cover this type of risk.\nScenario 3: Partially Insurable Risk\nSuppose you are a farmer worried about drought. Some types of crop insurance exist, but not all weather risks are insurable everywhere. If drought is a common, measurable risk in your area, insurers may offer coverage. But if the risk is too frequent or severe, or if there is not enough data, it may be uninsurable or only partially covered.\n\n\n\nUnderstanding risk—and knowing which risks are insurable—is essential for making smart decisions in life and business. It helps you focus your efforts and resources on the risks that matter most, and it allows you to use insurance and other tools effectively to protect yourself from financial loss.\nGood risk management means identifying your most important risks, understanding their likelihood and impact, and taking appropriate action. Sometimes that means accepting small risks, sometimes it means preparing for rare disasters, and sometimes it means transferring risk to an insurer. The key is to be proactive, informed, and realistic about what you can and cannot control.\n\n\n\n\n\n\n\n\n\n\n\n\nType of Risk\nExample\nHow to Manage\nInsurable?\n\n\n\n\nMinor\nPrinter out of paper\nAccept, Monitor\nNo\n\n\nRare but Serious\nEarthquake\nPlan, Transfer (insurance), Monitor\nSometimes (if data exists)\n\n\nFrequent, Low-Impact\nSmall IT glitches\nReduce, Monitor\nNo\n\n\nCritical\nCyberattack/data breach\nAct, Control, Respond, Monitor\nYes (with limits)\n\n\nFire\nFire in a bakery\nPrevent, Insure, Respond\nYes\n\n\nBusiness Failure\nBakery not popular\nBusiness planning, Diversify\nNo\n\n\nWear and Tear\nCar engine wears out\nMaintenance\nNo\n\n\nWar/Nuclear Disaster\nWar damages property\nGovernment aid (rare), Not insurable\nNo\n\n\n\n\nIn conclusion, risk is a complex but manageable part of life. By understanding its elements, types, and insurability, you can make better decisions, protect yourself and your business, and focus your energy where it matters most. Actuaries and insurers play a vital role in helping society manage risk, but everyone benefits from a deeper understanding of how risk works and how to respond to it."
  },
  {
    "objectID": "posts/understanding risk.html#key-elements-of-risk",
    "href": "posts/understanding risk.html#key-elements-of-risk",
    "title": "Understanding Risk",
    "section": "",
    "text": "When thinking about risk, there are two main questions to ask: - How likely is it to happen? (Probability) - How bad would it be if it happens? (Impact)\nManaging risk means understanding both how often something might go wrong and how serious it would be if it does."
  },
  {
    "objectID": "posts/understanding risk.html#types-of-risk-and-how-to-manage-them",
    "href": "posts/understanding risk.html#types-of-risk-and-how-to-manage-them",
    "title": "Understanding Risk",
    "section": "",
    "text": "Risks can be grouped based on their likelihood and impact. Here are four common types, and how you can manage each:\n\n\n\nThese are unlikely to happen and wouldn’t cause much harm if they did.\nHow to manage: - Accept the risk. These are often so small they aren’t worth much attention. - Monitor just in case. Keep an eye on them to make sure nothing changes.\nExample: Occasionally running out of printer paper at the office.\n\n\n\n\nUnlikely to occur, but if they do, the consequences could be severe.\nHow to manage: - Prepare contingency plans. Develop a plan for what to do if the risk happens. - Transfer the risk. Use insurance or contracts to share the risk with others. - Monitor regularly. Review the situation in case the likelihood increases.\nExample: An earthquake or other natural disaster.\n\n\n\n\nThese happen often, but their effects are minor.\nHow to manage: - Reduce how often they happen. Improve processes, provide training, or maintain equipment. - Keep records. Monitor how often these risks occur to spot any trends.\nExample: Minor technical glitches or small delays in routine tasks.\n\n\n\n\nBoth likely to happen and would have major consequences.\nHow to manage: - Act immediately. Address these risks with urgency. - Implement strong controls. Set up barriers or protections to reduce both likelihood and impact. - Develop and test response plans. Be ready to act if the risk materializes. - Monitor closely. Regularly review your controls and plans.\nExample: Cyberattacks that could expose sensitive data."
  },
  {
    "objectID": "posts/understanding risk.html#summary-table",
    "href": "posts/understanding risk.html#summary-table",
    "title": "Understanding Risk",
    "section": "",
    "text": "Type of Risk\nExample\nHow to Manage\n\n\n\n\nMinor\nPrinter out of paper\nAccept, Monitor\n\n\nRare but Serious\nEarthquake\nPlan, Transfer, Monitor\n\n\nFrequent, Low-Impact\nSmall IT glitches\nReduce, Monitor\n\n\nCritical\nCyberattack/data breach\nAct, Control, Respond, Monitor\n\n\n\n\nTip:\nGood risk management means focusing your efforts on the most important risks—those that could happen frequently or cause the most harm."
  },
  {
    "objectID": "posts/ml-Decoding Personality.html",
    "href": "posts/ml-Decoding Personality.html",
    "title": "Decision Trees: A Complete Guide for Beginners",
    "section": "",
    "text": "Decision trees are one of the most intuitive and powerful tools in machine learning and data science. They mimic the way humans make decisions: by asking a series of questions and following the answers down different paths. In this article, we’ll break down what decision trees are, define the most important terms, explore the different types of decision trees based on the kind of output they produce, and explain the key metrics used to evaluate them. By the end, you’ll have a clear understanding of how decision trees work and how to use them for both classification and regression problems.\n\n\n\nA decision tree is a flowchart-like structure used to make decisions or predictions. Each internal node of the tree represents a test or question about a feature (for example, “Is age &gt; 30?”), each branch represents the outcome of the test, and each leaf node represents a final decision or prediction. Decision trees can be used for both classification (predicting categories) and regression (predicting numbers).\nImagine you want to decide whether to play tennis based on the weather. A decision tree might first ask, “Is it sunny?” If yes, it might then ask, “Is the humidity high?” and so on, until it reaches a decision like “Play” or “Don’t play.”\n\n\n\nBefore we dive deeper, let’s define some important terms:\n\nRoot Node: The top node of the tree, where the first split or question is made.\nInternal Node: Any node that splits into further branches (not a leaf).\nLeaf Node (Terminal Node): The end node that gives the final output (class or value).\nBranch: A path from one node to another, representing the outcome of a test.\nSplit: The process of dividing a node into two or more sub-nodes based on a feature.\nFeature (Attribute): A variable or column in your dataset used to split the data.\nDepth: The number of levels in the tree from the root to the deepest leaf.\n\n\n\n\nDecision trees are divided into two main types, depending on the nature of the output variable:\n\n\nClassification trees are used when the target variable is categorical—that is, when you want to predict a class or label (such as “spam” vs. “not spam,” or “disease” vs. “no disease”). At each node, the tree asks a question that splits the data into groups that are more homogeneous with respect to the target class.\nExample: Suppose you want to predict whether a loan applicant will default (“Yes” or “No”). The tree might split on features like income, credit score, or employment status, eventually leading to a prediction at the leaf node.\n\n\nTo decide the best way to split the data at each node, classification trees use metrics that measure how “pure” or homogeneous the resulting groups are. The most common metrics are:\n\nGini Impurity: Measures how often a randomly chosen element would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the node. Lower Gini means purer nodes.\nEntropy (Information Gain): Measures the amount of disorder or uncertainty. Splits that reduce entropy the most are preferred.\n\nHow to choose splits: At each node, the algorithm tries all possible splits and chooses the one that results in the greatest reduction in impurity (Gini or Entropy).\nEvaluation Metrics: After building the tree, we evaluate its performance using metrics such as: - Accuracy: The proportion of correct predictions. - Precision, Recall, F1 Score: Useful for imbalanced datasets. - Confusion Matrix: Shows the counts of true positives, false positives, etc.\n\n\n\n\nRegression trees are used when the target variable is continuous or numerical (such as predicting house prices or temperatures). Instead of predicting a class, the tree predicts a number.\nExample: Suppose you want to predict the price of a house based on features like size, location, and number of bedrooms. The regression tree splits the data at each node to minimize the difference between the predicted and actual values.\n\n\nTo choose the best splits, regression trees use metrics that measure how well the split reduces the variability of the target variable. The most common metrics are:\n\nMean Squared Error (MSE): The average of the squared differences between predicted and actual values.\nMean Absolute Error (MAE): The average of the absolute differences between predicted and actual values.\n\nHow to choose splits: At each node, the algorithm tries all possible splits and chooses the one that results in the greatest reduction in error (MSE or MAE).\nEvaluation Metrics: After building the tree, we evaluate its performance using metrics such as: - R-squared (R²): Measures how well the model explains the variability of the target. - Root Mean Squared Error (RMSE): The square root of MSE, in the same units as the target.\n\n\n\n\n\nAdvantages: - Easy to understand and interpret. - Can handle both numerical and categorical data. - Require little data preparation. - Can model non-linear relationships.\nLimitations: - Prone to overfitting (creating trees that are too complex and fit the training data too closely). - Can be unstable—small changes in data can lead to different trees. - Less accurate than some other algorithms (like random forests or boosting) on complex problems.\n\n\n\nLet’s walk through a practical example using R, where we predict whether a person is an introvert or extrovert using a decision tree. We’ll cover every step: reading the data, cleaning it, splitting into training and test sets, building the tree, evaluating it, and pruning for better performance.\n\n\nFirst, we load the necessary libraries and read the dataset directly from the provided URL. We’ll use Quarto chunk options to suppress warnings and messages for a cleaner output.\n\nlibrary(readr)\nlibrary(janitor)\nlibrary(dplyr)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(caret) # for train/test split\n\n# Read the data\ndf &lt;- read_csv(\"https://raw.githubusercontent.com/IKSHRESTHA/Actuarial-Reflections/refs/heads/main/data/06272925/personality_datasert.csv\") |&gt; \n  janitor::clean_names()\n\n# Inspect the data\nstr(df)\n\nspc_tbl_ [2,900 × 8] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ time_spent_alone         : num [1:2900] 4 9 9 0 3 1 4 2 10 0 ...\n $ stage_fear               : chr [1:2900] \"No\" \"Yes\" \"Yes\" \"No\" ...\n $ social_event_attendance  : num [1:2900] 4 0 1 6 9 7 9 8 1 8 ...\n $ going_outside            : num [1:2900] 6 0 2 7 4 5 3 4 3 6 ...\n $ drained_after_socializing: chr [1:2900] \"No\" \"Yes\" \"Yes\" \"No\" ...\n $ friends_circle_size      : num [1:2900] 13 0 5 14 8 6 7 7 0 13 ...\n $ post_frequency           : num [1:2900] 5 3 2 8 5 6 7 8 3 8 ...\n $ personality              : chr [1:2900] \"Extrovert\" \"Introvert\" \"Introvert\" \"Extrovert\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Time_spent_Alone = col_double(),\n  ..   Stage_fear = col_character(),\n  ..   Social_event_attendance = col_double(),\n  ..   Going_outside = col_double(),\n  ..   Drained_after_socializing = col_character(),\n  ..   Friends_circle_size = col_double(),\n  ..   Post_frequency = col_double(),\n  ..   Personality = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nsummary(df)\n\n time_spent_alone  stage_fear        social_event_attendance going_outside\n Min.   : 0.000   Length:2900        Min.   : 0.000          Min.   :0    \n 1st Qu.: 2.000   Class :character   1st Qu.: 2.000          1st Qu.:1    \n Median : 4.000   Mode  :character   Median : 3.963          Median :3    \n Mean   : 4.506                      Mean   : 3.963          Mean   :3    \n 3rd Qu.: 7.000                      3rd Qu.: 6.000          3rd Qu.:5    \n Max.   :11.000                      Max.   :10.000          Max.   :7    \n drained_after_socializing friends_circle_size post_frequency  \n Length:2900               Min.   : 0.000      Min.   : 0.000  \n Class :character          1st Qu.: 3.000      1st Qu.: 1.000  \n Mode  :character          Median : 5.000      Median : 3.000  \n                           Mean   : 6.269      Mean   : 3.565  \n                           3rd Qu.:10.000      3rd Qu.: 6.000  \n                           Max.   :15.000      Max.   :10.000  \n personality       \n Length:2900       \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n\nThe output above shows the structure and summary statistics of the dataset. You can see the variable types, ranges, and a quick overview of the data. This helps us understand what features are available and if there are any obvious data quality issues.\n\n\n\nWe’ll make sure the target variable (personality) is a factor, and check for missing values. We’ll also suppress warnings and messages in this chunk.\n\n# Convert target to factor\ndf$personality &lt;- as.factor(df$personality)\n\n# Check for missing values\ncolSums(is.na(df))\n\n         time_spent_alone                stage_fear   social_event_attendance \n                        0                         0                         0 \n            going_outside drained_after_socializing       friends_circle_size \n                        0                         0                         0 \n           post_frequency               personality \n                        0                         0 \n\n\nThe output will show the number of missing values in each column. If all values are zero, there are no missing data to worry about. If not, you may need to handle them before modeling.\n\n\n\nWe’ll split the data into 70% training and 30% testing sets to evaluate our model’s performance on unseen data.\n\nset.seed(123) # for reproducibility\ntrain_index &lt;- createDataPartition(df$personality, p = 0.7, list = FALSE)\ntrain_data &lt;- df[train_index, ]\ntest_data &lt;- df[-train_index, ]\n\nThis step ensures that our model is trained on one portion of the data and tested on another, helping us assess how well it generalizes to new cases.\n\n\n\nNow, we’ll build a classification tree to predict personality using all other variables.\n\ntree_model &lt;- rpart(personality ~ ., data = train_data, method = \"class\", cp = 0.01)\n\n# Visualize the tree\nrpart.plot(tree_model)\n\n\n\n\n\n\n\n\nThe plot above shows the structure of the decision tree. Each node represents a split based on a feature, and the leaves show the predicted class (introvert or extrovert).\n\n\n\nWe’ll use the test set to see how well our tree predicts introverts vs. extroverts.\n\npred &lt;- predict(tree_model, test_data, type = \"class\")\nconfusionMatrix(pred, test_data$personality)\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  Extrovert Introvert\n  Extrovert       412        29\n  Introvert        35       393\n                                          \n               Accuracy : 0.9264          \n                 95% CI : (0.9069, 0.9428)\n    No Information Rate : 0.5144          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.8526          \n                                          \n Mcnemar's Test P-Value : 0.532           \n                                          \n            Sensitivity : 0.9217          \n            Specificity : 0.9313          \n         Pos Pred Value : 0.9342          \n         Neg Pred Value : 0.9182          \n             Prevalence : 0.5144          \n         Detection Rate : 0.4741          \n   Detection Prevalence : 0.5075          \n      Balanced Accuracy : 0.9265          \n                                          \n       'Positive' Class : Extrovert       \n                                          \n\n\nThe confusion matrix output will display the number of correct and incorrect predictions for each class. Accuracy, sensitivity, and specificity are also shown, helping you judge the model’s performance.\n\n\n\nDecision trees can overfit, so pruning helps simplify the tree and improve generalization. We’ll use the complexity parameter (cp) to prune.\n\n# Find optimal cp value\nprintcp(tree_model)\n\n\nClassification tree:\nrpart(formula = personality ~ ., data = train_data, method = \"class\", \n    cp = 0.01)\n\nVariables actually used in tree construction:\n[1] drained_after_socializing stage_fear               \n\nRoot node error: 987/2031 = 0.48597\n\nn= 2031 \n\n        CP nsplit rel error  xerror     xstd\n1 0.855117      0   1.00000 1.00000 0.022821\n2 0.016211      1   0.14488 0.14488 0.011681\n3 0.010000      2   0.12867 0.12969 0.011096\n\n# Choose the cp with lowest cross-validated error\nbest_cp &lt;- tree_model$cptable[which.min(tree_model$cptable[,\"xerror\"]), \"CP\"]\n\n# Prune the tree\npruned_tree &lt;- prune(tree_model, cp = best_cp)\n\n# Visualize pruned tree\nrpart.plot(pruned_tree)\n\n\n\n\n\n\n\n# Evaluate pruned tree\npruned_pred &lt;- predict(pruned_tree, test_data, type = \"class\")\nconfusionMatrix(pruned_pred, test_data$personality)\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  Extrovert Introvert\n  Extrovert       412        29\n  Introvert        35       393\n                                          \n               Accuracy : 0.9264          \n                 95% CI : (0.9069, 0.9428)\n    No Information Rate : 0.5144          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.8526          \n                                          \n Mcnemar's Test P-Value : 0.532           \n                                          \n            Sensitivity : 0.9217          \n            Specificity : 0.9313          \n         Pos Pred Value : 0.9342          \n         Neg Pred Value : 0.9182          \n             Prevalence : 0.5144          \n         Detection Rate : 0.4741          \n   Detection Prevalence : 0.5075          \n      Balanced Accuracy : 0.9265          \n                                          \n       'Positive' Class : Extrovert       \n                                          \n\n\nAfter pruning, the tree is simpler and less likely to overfit. The new confusion matrix shows how well the pruned tree performs on the test data. Compare this to the previous results to see if pruning improved generalization.\n\n\n\n\nWe loaded and cleaned the data (with warnings suppressed for clarity).\nSplit it into training and test sets.\nBuilt and visualized a decision tree to predict personality type.\nEvaluated its performance with a confusion matrix.\nPruned the tree and compared results.\n\nThis step-by-step approach helps you understand not just how to build a decision tree, but also how to interpret the output and ensure it performs well on new, unseen data."
  },
  {
    "objectID": "posts/ml-Decoding Personality.html#introduction",
    "href": "posts/ml-Decoding Personality.html#introduction",
    "title": "Decision Trees: A Complete Guide for Beginners",
    "section": "",
    "text": "Decision trees are one of the most intuitive and powerful tools in machine learning and data science. They mimic the way humans make decisions: by asking a series of questions and following the answers down different paths. In this article, we’ll break down what decision trees are, define the most important terms, explore the different types of decision trees based on the kind of output they produce, and explain the key metrics used to evaluate them. By the end, you’ll have a clear understanding of how decision trees work and how to use them for both classification and regression problems."
  },
  {
    "objectID": "posts/ml-Decoding Personality.html#why-eda",
    "href": "posts/ml-Decoding Personality.html#why-eda",
    "title": "Decoding Personality: ML Models as Your Guide to Introverts vs. Extroverts",
    "section": "",
    "text": "EDA helps us understand our variables.\nHelps us detect data Quality Issues ( Missing Values, Outliers )\nHelps us uncover hidden relationships\nHelp us Validate assumptions\nHelp us in feature engineering\n\nLets start now exploring the data. First thing First, we need to import library that we are going to use for this project:\nAfter loaing the library, we will read the data."
  },
  {
    "objectID": "posts/ml-Decoding Personality.html#about-the-data",
    "href": "posts/ml-Decoding Personality.html#about-the-data",
    "title": "Decoding Personality: ML Models as Your Guide to Introverts vs. Extroverts",
    "section": "",
    "text": "The data has been downloaded from the kaggle and you can find about the data more on here:\n\npersonality_dataset &lt;-readr::read_csv(\"https://raw.githubusercontent.com/IKSHRESTHA/Actuarial-Reflections/refs/heads/main/data/06272925/personality_datasert.csv\")\n\n`curl` package not installed, falling back to using `url()`\nRows: 2900 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Stage_fear, Drained_after_socializing, Personality\ndbl (5): Time_spent_Alone, Social_event_attendance, Going_outside, Friends_c...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# cleaning the columns name \ndf &lt;- personality_dataset |&gt;\n  janitor::clean_names()\n\ncolnames(df)\n\n[1] \"time_spent_alone\"          \"stage_fear\"               \n[3] \"social_event_attendance\"   \"going_outside\"            \n[5] \"drained_after_socializing\" \"friends_circle_size\"      \n[7] \"post_frequency\"            \"personality\"              \n\n\n\nlibrary(extrafont)  # For font consistency\n\nRegistering fonts with R\n\ntheme_actuarial &lt;- function(base_size = 12, \n                            base_family = \"mono\",\n                            title_position = \"middle\",\n                            border_color = \"black\") {\n  \n  # Load monospaced font (install first if needed)\n  extrafont::loadfonts(quiet = TRUE)\n  \n  theme_minimal(base_size = base_size, base_family = base_family) %+replace%\n    theme(\n      # Text elements\n      text = element_text(family = base_family, color = \"black\"),\n      title = element_text(face = \"bold\", size = base_size * 1.2),\n      plot.title = element_text(\n        hjust = 0.5, \n        margin = margin(b = base_size),\n        size = base_size * 1.4\n      ),\n      plot.subtitle = element_text(\n        hjust = 0.5, \n        margin = margin(b = base_size)\n      ),\n      plot.caption = element_text(\n        hjust = 1, \n        size = base_size * 0.8,\n        color = \"grey40\"\n      ),\n      \n      # Axis elements\n      axis.title = element_text(\n        face = \"bold\", \n        color = \"black\",\n        size = base_size * 1.1\n      ),\n      axis.title.x = element_text(\n        margin = margin(t = base_size * 0.5),\n        hjust = title_position\n      ),\n      axis.title.y = element_text(\n        margin = margin(r = base_size * 0.5),\n        angle = 90,\n        hjust = title_position\n      ),\n      axis.text = element_text(color = \"grey30\"),\n      axis.line = element_line(\n        color = border_color, \n        linewidth = 0.8\n      ),\n      \n      # Panel elements\n      panel.grid.major = element_line(\n        color = \"grey92\", \n        linewidth = 0.3\n      ),\n      panel.grid.minor = element_blank(),\n      panel.background = element_rect(fill = \"white\", color = NA),\n      panel.border = element_blank(),\n      \n      # Legend elements\n      legend.title = element_text(face = \"bold\"),\n      legend.background = element_rect(fill = \"white\", color = NA),\n      legend.key = element_rect(fill = \"white\", color = NA),\n      \n      # Plot elements\n      plot.background = element_rect(fill = \"white\", color = NA),\n      plot.margin = margin(base_size, base_size, base_size, base_size),\n      strip.background = element_rect(fill = \"grey95\", color = NA),\n      strip.text = element_text(face = \"bold\")\n    )\n}\n\n\n#df_summary=rstatix::get_summary_stats(df,type=\"full\")\n#df_summary"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Actuarial-Reflections",
    "section": "",
    "text": "Krishna Shrestha\n\n\n\n\n\n\n&lt;a href=\"https://tenor.com/view/pikachu-pokemon-waving-wave-hi-gif-16091246\"&gt;Pikachu Pokemon Sticker&lt;/a&gt;\nfrom &lt;a href=\"https://tenor.com/search/pikachu-stickers\"&gt;Pikachu Stickers&lt;/a&gt;\n\n\n\n\n\n\nActuarial Reflections Personal blog by Krishna Shrestha — sharing insights, tutorials, and reflections on actuarial science, data science, and professional growth. Explore the latest posts below!\n\n\n\n\n\n\n\n\n View All Blogs \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Trees: A Complete Guide for Beginners\n\n\n\n\n\n\nmachine learning\n\n\ndecision trees\n\n\nclassification\n\n\nregression\n\n\n\n\n\n\n\n\n\nJul 12, 2025\n\n\nKrishna Kumar Shrestha\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Experience Study\n\n\n\n\n\n\nactuarial science\n\n\nlife insurance\n\n\nnon-life insurance\n\n\n\n\n\n\n\n\n\nJun 26, 2025\n\n\nKrishna Kumar Shrestha\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Become an Actuary in Nepal\n\n\n\n\n\n\nActuary\n\n\nNepal\n\n\nCareer\n\n\n\n\n\n\n\n\n\nMay 20, 2025\n\n\nKrishna Kumar Shrestha\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Risk\n\n\n\n\n\n\nactuarial science\n\n\nlife insurance\n\n\nnon-life insurance\n\n\nrisk\n\n\nrisk Managment\n\n\n\n\n\n\n\n\n\nApr 26, 2025\n\n\nKrishna Kumar Shrestha\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Important\n\n\n\nOpen to Work:\nI am actively seeking opportunities in actuarial science roles in the USA. If you know of any positions or have networking leads, please feel free to reach out!\nHi, I’m Krishna Kumar Shrestha—an aspiring actuary passionate about using data, analytics, and technology to solve real-world problems in insurance and risk management.\nI am currently pursuing my Master of Science in Professional Science (Actuarial Science) at Middle Tennessee State University, maintaining a GPA of 3.92. My academic journey began in Nepal, where I earned my Bachelor of Mathematical Sciences (Actuarial Science) from Tribhuvan University. Along the way, I’ve passed several Society of Actuaries (SOA) exams:\n✅ Exam P\n✅ Exam FM\n✅ Exam FAM\n✅ Exam ALTAM\n✅ Exam SRM"
  },
  {
    "objectID": "about.html#professional-experience",
    "href": "about.html#professional-experience",
    "title": "About Me",
    "section": "Professional Experience",
    "text": "Professional Experience\nMy hands-on experience spans actuarial consulting, teaching, analytics, and insurtech. Currently, I serve as a:\n👨‍🏫 Graduate Teaching Assistant, MTSU (Present):\nSupporting actuarial coursework, instruction, mentoring, and applied problem-solving.\n🧮 Senior Actuarial Analyst, Principal Risk Consulting:\nDeveloped pricing models for life insurance and led actuarial valuations under international accounting standards.\n👨‍💻 Assistant Lecturer, Tribhuvan University:\nTaught R programming and Excel for actuarial applications; contributed to curriculum development and accreditation.\n🚀 Consultant, eBeema (Insurtech Startup):\nSupported digital insurance platform launches and enhanced user experiences.\n📊 Data Analyst Intern, Numeric Mind:\nWorked on real data projects early in my career."
  },
  {
    "objectID": "about.html#skills-certifications",
    "href": "about.html#skills-certifications",
    "title": "About Me",
    "section": "Skills & Certifications",
    "text": "Skills & Certifications\n🐍 Programming: R, Python, Excel\n📊 Actuarial Modeling & Data Analytics\n🏆 Certifications:\n- Data Analyst Professional (DataCamp)\n- Winner, IFoA R Number Modeling Hackathon (2021)\n- Finalist, Kislay Actuarial Hackathon (2020)"
  },
  {
    "objectID": "about.html#leadership-community",
    "href": "about.html#leadership-community",
    "title": "About Me",
    "section": "Leadership & Community",
    "text": "Leadership & Community\n🤝 Secretary, Actuarial Society of Nepal\nOrganized training, networking events, and knowledge-sharing for the actuarial community.\n🌐 Collaborated with regulators and international bodies (SOA, IAA, UNDP) to support policy and actuarial growth in Nepal."
  },
  {
    "objectID": "about.html#my-approach",
    "href": "about.html#my-approach",
    "title": "About Me",
    "section": "My Approach",
    "text": "My Approach\nI believe actuarial science is not just about numbers—it’s about making a positive impact on people’s lives by quantifying risk and guiding decisions with integrity and insight. Whether building pricing models, teaching, or launching digital solutions, I bring curiosity, collaboration, and a commitment to professional excellence."
  },
  {
    "objectID": "about.html#lets-connect",
    "href": "about.html#lets-connect",
    "title": "About Me",
    "section": "Let’s Connect",
    "text": "Let’s Connect\n\nLinkedIn\n📧 krishnakumarshrestha00@gmail.com\n\n\nExplore my blog for reflections on actuarial topics, data science, and my professional journey from Nepal to the U.S. and beyond!"
  },
  {
    "objectID": "all-blogs.html",
    "href": "all-blogs.html",
    "title": "All Blog Posts",
    "section": "",
    "text": "Below you can find all blog posts published on Actuarial Reflections.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nDecision Trees: A Complete Guide for Beginners\n\n\n\n\n\n\nmachine learning\n\n\ndecision trees\n\n\nclassification\n\n\nregression\n\n\n\n\n\n\n\n\n\nJul 12, 2025\n\n\nKrishna Kumar Shrestha\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Experience Study\n\n\n\n\n\n\nactuarial science\n\n\nlife insurance\n\n\nnon-life insurance\n\n\n\n\n\n\n\n\n\nJun 26, 2025\n\n\nKrishna Kumar Shrestha\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Become an Actuary in Nepal\n\n\n\n\n\n\nActuary\n\n\nNepal\n\n\nCareer\n\n\n\n\n\n\n\n\n\nMay 20, 2025\n\n\nKrishna Kumar Shrestha\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Risk\n\n\n\n\n\n\nactuarial science\n\n\nlife insurance\n\n\nnon-life insurance\n\n\nrisk\n\n\nrisk Managment\n\n\n\n\n\n\n\n\n\nApr 26, 2025\n\n\nKrishna Kumar Shrestha\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/actuary-in-nepal.html",
    "href": "posts/actuary-in-nepal.html",
    "title": "How to Become an Actuary in Nepal",
    "section": "",
    "text": "Summary: This is the most comprehensive guide for aspiring actuaries in Nepal. It covers every aspect of the profession, from education and exams to career paths, regulatory context, and professional development. Use the table of contents to navigate."
  },
  {
    "objectID": "posts/Understanding Experiance Study.html",
    "href": "posts/Understanding Experiance Study.html",
    "title": "Understanding Experience Study",
    "section": "",
    "text": "Based on the article by Matthew Dunscombe and Alexander Zaidlin\n\n\n\nThis document summarizes the key points from the article “Experience Studies – Understanding the Past While Planning for the Future,” which explores the critical role of experience studies in actuarial science and modern insurance. Experience studies analyze actual versus expected insurance events (such as deaths, lapses, and claims) within defined populations. The process supports actuaries in understanding trends, identifying risk drivers, refining assumptions, and complying with evolving financial standards.\n\n\n\n\n\n📊 Foundational Role: Experience studies are fundamental to actuarial work, dating back to the 17th century.\n🔍 Core Metric: The comparison of actual insurance events to expected figures produces the actual-to-expected (A/E) ratio.\n🛠️ Seven-Step Process: Steps include data gathering, preparation, exposure calculation, actual/expected comparison, aggregation, analysis, validation, and reporting.\n📈 Trend Analysis: Identifying data trends and outliers is vital for setting accurate assumptions.\n⚖️ Credibility and Adjustment: Credibility methods and manual adjustments help refine and stabilize results.\n🌍 External Factors: External variables and incurred-but-not-reported (IBNR) claims add complexity and require expert judgment.\n💻 Technology: Tools like SQL Server and SAS enable large-scale, efficient experience studies in addition to traditional Excel-based approaches.\n\n\n\n\n\n\n\nExperience studies have shaped actuarial science for centuries—beginning with Edmund Halley’s annuity analysis. Over time, these studies evolved from simple mortality tables to complex, data-driven models essential for modern pricing, reserving, and regulatory compliance.\n\n\n\nCareful selection between policy snapshot datasets and transactional records is critical. Snapshot datasets provide a static policy view, while transactional datasets offer granular, event-level insights. Collaboration with IT and claims departments is vital to ensure data quality, making data preparation the most labor-intensive step.\n\n\n\nCalculating exposure quantifies the risk period for insurance events, enabling actuaries to derive rates by count and by amount (the latter reflecting financial impact). The choice between calendar year and policy year studies affects how exposure is segmented and analyzed.\n\n\n\nThe main analytical output is the A/E ratio, which compares observed claims to expected figures (from industry tables or internal assumptions). This ratio highlights deviations, guiding assumption changes or further analysis.\n\n\n\nGrouping results (by gender, product, etc.) enables actionable insights. Credibility theory determines the statistical reliability of groupings and whether to use benchmarks or granular analysis. Advanced methods like generalized linear models (GLMs) and Bayesian approaches enhance credibility assessments.\n\n\n\nActuaries must apply judgment in adjusting outputs for volatility and external events. Peer review and stakeholder feedback ensure assumptions are robust. Trend analysis links changes to underwriting, economic shifts, or product mix, informing future projections.\n\n\n\nNon-core influences like regulatory changes and market conditions can distort results. IBNR and in-course-of-settlement (ICOS) claims create uncertainty, requiring careful estimation to avoid understating actual experience.\n\n\n\nModern tools (SQL Server, SAS, etc.) handle large volumes and complex calculations more efficiently than traditional spreadsheet tools, enabling faster and more reproducible analyses.\n\n\n\nRigorous validation (reconciliation, sampling, analytical review) ensures accuracy. Documentation of methodology, assumptions, and findings supports transparency and sound decision-making.\n\n\n\nEffective studies require coordination between actuaries, claims, underwriting, IT, and business units. This teamwork improves data accuracy, result interpretation, and agreement on adjustments.\n\n\n\n\n\nExperience studies are the empirical foundation of actuarial analysis. While the main concept—comparing actual to expected events—is straightforward, practical implementation involves complex data preparation, nuanced exposure calculation, trend/outlier analysis, and expert judgment.\nKey aspects include: - Data Preparation: The most resource-intensive step, involving cleansing, linking, and validating from various sources. The chosen data structure influences study design and insights. - Exposure Measurement: Accurate measurement underpins rates and A/E ratios, revealing alignment or deviation from expectations. - Trend and Outlier Detection: Helps refine assumptions and uncover extraordinary events or data issues. - Credibility and Modeling: Sophisticated statistical techniques balance observed data with prior information. - Manual Adjustments: Required to smooth volatility and reflect external events—peer-reviewed for transparency. - Handling External Factors: Considered through advanced statistical methods and judgment. - IBNR and ICOS: Properly estimating late or unsettled claims is essential for accuracy. - Technology: Modern platforms streamline large, complex studies, letting actuaries focus on interpretation. - Documentation & Validation: Ensures stakeholder confidence and future usability. - Collaboration: Critical for ensuring data accuracy and relevance of findings.\n\n\n\n\nExperience studies remain vital for pricing, reserving, and risk management in insurance. Their success hinges on robust data preparation, accurate measurement, sound statistical practice, effective technology, and strong cross-department collaboration. Comprehensive documentation and validation underpin credibility, ensuring that experience studies remain a cornerstone of informed, data-driven actuarial decision-making."
  },
  {
    "objectID": "posts/Understanding Experiance Study.html#overview",
    "href": "posts/Understanding Experiance Study.html#overview",
    "title": "Understanding Experience Study",
    "section": "",
    "text": "This document summarizes the key points from the article “Experience Studies – Understanding the Past While Planning for the Future,” which explores the critical role of experience studies in actuarial science and modern insurance. Experience studies analyze actual versus expected insurance events (such as deaths, lapses, and claims) within defined populations. The process supports actuaries in understanding trends, identifying risk drivers, refining assumptions, and complying with evolving financial standards."
  },
  {
    "objectID": "posts/Understanding Experiance Study.html#highlights",
    "href": "posts/Understanding Experiance Study.html#highlights",
    "title": "Understanding Experience Study",
    "section": "",
    "text": "📊 Foundational Role: Experience studies are fundamental to actuarial work, dating back to the 17th century.\n🔍 Core Metric: The comparison of actual insurance events to expected figures produces the actual-to-expected (A/E) ratio.\n🛠️ Seven-Step Process: Steps include data gathering, preparation, exposure calculation, actual/expected comparison, aggregation, analysis, validation, and reporting.\n📈 Trend Analysis: Identifying data trends and outliers is vital for setting accurate assumptions.\n⚖️ Credibility and Adjustment: Credibility methods and manual adjustments help refine and stabilize results.\n🌍 External Factors: External variables and incurred-but-not-reported (IBNR) claims add complexity and require expert judgment.\n💻 Technology: Tools like SQL Server and SAS enable large-scale, efficient experience studies in addition to traditional Excel-based approaches."
  },
  {
    "objectID": "posts/Understanding Experiance Study.html#key-insights",
    "href": "posts/Understanding Experiance Study.html#key-insights",
    "title": "Understanding Experience Study",
    "section": "",
    "text": "Experience studies have shaped actuarial science for centuries—beginning with Edmund Halley’s annuity analysis. Over time, these studies evolved from simple mortality tables to complex, data-driven models essential for modern pricing, reserving, and regulatory compliance.\n\n\n\nCareful selection between policy snapshot datasets and transactional records is critical. Snapshot datasets provide a static policy view, while transactional datasets offer granular, event-level insights. Collaboration with IT and claims departments is vital to ensure data quality, making data preparation the most labor-intensive step.\n\n\n\nCalculating exposure quantifies the risk period for insurance events, enabling actuaries to derive rates by count and by amount (the latter reflecting financial impact). The choice between calendar year and policy year studies affects how exposure is segmented and analyzed.\n\n\n\nThe main analytical output is the A/E ratio, which compares observed claims to expected figures (from industry tables or internal assumptions). This ratio highlights deviations, guiding assumption changes or further analysis.\n\n\n\nGrouping results (by gender, product, etc.) enables actionable insights. Credibility theory determines the statistical reliability of groupings and whether to use benchmarks or granular analysis. Advanced methods like generalized linear models (GLMs) and Bayesian approaches enhance credibility assessments.\n\n\n\nActuaries must apply judgment in adjusting outputs for volatility and external events. Peer review and stakeholder feedback ensure assumptions are robust. Trend analysis links changes to underwriting, economic shifts, or product mix, informing future projections.\n\n\n\nNon-core influences like regulatory changes and market conditions can distort results. IBNR and in-course-of-settlement (ICOS) claims create uncertainty, requiring careful estimation to avoid understating actual experience.\n\n\n\nModern tools (SQL Server, SAS, etc.) handle large volumes and complex calculations more efficiently than traditional spreadsheet tools, enabling faster and more reproducible analyses.\n\n\n\nRigorous validation (reconciliation, sampling, analytical review) ensures accuracy. Documentation of methodology, assumptions, and findings supports transparency and sound decision-making.\n\n\n\nEffective studies require coordination between actuaries, claims, underwriting, IT, and business units. This teamwork improves data accuracy, result interpretation, and agreement on adjustments."
  },
  {
    "objectID": "posts/Understanding Experiance Study.html#extended-discussion",
    "href": "posts/Understanding Experiance Study.html#extended-discussion",
    "title": "Understanding Experience Study",
    "section": "",
    "text": "Experience studies are the empirical foundation of actuarial analysis. While the main concept—comparing actual to expected events—is straightforward, practical implementation involves complex data preparation, nuanced exposure calculation, trend/outlier analysis, and expert judgment.\nKey aspects include: - Data Preparation: The most resource-intensive step, involving cleansing, linking, and validating from various sources. The chosen data structure influences study design and insights. - Exposure Measurement: Accurate measurement underpins rates and A/E ratios, revealing alignment or deviation from expectations. - Trend and Outlier Detection: Helps refine assumptions and uncover extraordinary events or data issues. - Credibility and Modeling: Sophisticated statistical techniques balance observed data with prior information. - Manual Adjustments: Required to smooth volatility and reflect external events—peer-reviewed for transparency. - Handling External Factors: Considered through advanced statistical methods and judgment. - IBNR and ICOS: Properly estimating late or unsettled claims is essential for accuracy. - Technology: Modern platforms streamline large, complex studies, letting actuaries focus on interpretation. - Documentation & Validation: Ensures stakeholder confidence and future usability. - Collaboration: Critical for ensuring data accuracy and relevance of findings."
  },
  {
    "objectID": "posts/Understanding Experiance Study.html#conclusion",
    "href": "posts/Understanding Experiance Study.html#conclusion",
    "title": "Understanding Experience Study",
    "section": "",
    "text": "Experience studies remain vital for pricing, reserving, and risk management in insurance. Their success hinges on robust data preparation, accurate measurement, sound statistical practice, effective technology, and strong cross-department collaboration. Comprehensive documentation and validation underpin credibility, ensuring that experience studies remain a cornerstone of informed, data-driven actuarial decision-making."
  },
  {
    "objectID": "posts/actuary-in-nepal.html#universities-and-programs",
    "href": "posts/actuary-in-nepal.html#universities-and-programs",
    "title": "How to Become an Actuary in Nepal",
    "section": "Universities and Programs",
    "text": "Universities and Programs\nThe flagship program for actuarial education in Nepal is the Bachelor in Mathematical Sciences (Actuarial Science) offered by the School of Mathematical Sciences (SMS) at Tribhuvan University (TU). This program is designed to align with international actuarial syllabi, particularly those of the Society of Actuaries (SOA) and the Institute and Faculty of Actuaries (IFoA). The curriculum covers core areas such as probability, statistics, financial mathematics, life contingencies, risk theory, and computer programming. Students also gain exposure to economics, finance, and business, ensuring a well-rounded education.\nOther universities in Nepal may offer degrees in mathematics, statistics, or economics, which can also serve as a pathway into actuarial science. However, the TU program is unique in its direct alignment with professional actuarial exams and its recognition by international bodies. Students from other universities often supplement their education with self-study or online courses to prepare for actuarial exams."
  },
  {
    "objectID": "posts/actuary-in-nepal.html#course-outlines-and-syllabi",
    "href": "posts/actuary-in-nepal.html#course-outlines-and-syllabi",
    "title": "How to Become an Actuary in Nepal",
    "section": "Course Outlines and Syllabi",
    "text": "Course Outlines and Syllabi\nThe actuarial science curriculum at Tribhuvan University is comprehensive and rigorous. Key courses typically include: - Probability and Statistics - Financial Mathematics - Life Insurance Mathematics - Risk Theory - Survival Models - Stochastic Processes - Economics and Finance - Actuarial Modeling - Computer Programming (often in R snd Python) - Data Analysis and Machine Learning (in advanced years)\nThe program is structured to help students prepare for the preliminary exams of international actuarial societies. Many courses are mapped to the Validation by Educational Experience (VEE) requirements of the SOA , allowing students to earn exemptions or credits toward professional qualifications. In addition to classroom learning, students are encouraged to participate in research projects, internships, and industry seminars."
  },
  {
    "objectID": "posts/actuary-in-nepal.html#scholarships-and-financial-aid",
    "href": "posts/actuary-in-nepal.html#scholarships-and-financial-aid",
    "title": "How to Become an Actuary in Nepal",
    "section": "Scholarships and Financial Aid",
    "text": "Scholarships and Financial Aid\nFinancing an actuarial education can be a concern for many students. Tribhuvan University, as a public institution, offers relatively affordable tuition compared to private universities. Additionally, merit-based scholarships are available for high-performing students, both at the time of admission and during the course of study. Some scholarships are specifically targeted at students from underrepresented regions or backgrounds.\nProfessional actuarial societies, such as the SOA and IFoA, occasionally offer scholarships or exam fee waivers for students in developing countries, including Nepal. It is advisable to regularly check the official websites of these organizations for announcements. Local insurance companies and consulting firms may also sponsor promising students, especially those who demonstrate strong academic performance and a commitment to the profession."
  },
  {
    "objectID": "posts/actuary-in-nepal.html#how-to-prepare-in-high-school",
    "href": "posts/actuary-in-nepal.html#how-to-prepare-in-high-school",
    "title": "How to Become an Actuary in Nepal",
    "section": "How to Prepare in High School",
    "text": "How to Prepare in High School\nAspiring actuaries should begin preparing as early as high school. A strong foundation in mathematics is essential, so students should take advanced courses in mathematics, statistics, and, if available, economics or business studies. Participation in math competitions, science fairs, or computer programming clubs can help develop problem-solving skills and demonstrate commitment to quantitative fields.\nDeveloping proficiency in English is also important, as most actuarial exams and study materials are in English. Students should practice reading technical texts, writing reports, and communicating complex ideas clearly. Familiarity with computers and basic programming concepts will be beneficial, as modern actuarial work increasingly relies on data analysis and modeling software.\nFinally, students should seek out mentors—teachers, university students, or professionals—who can provide guidance and encouragement. Attending career fairs, university open days, or online webinars about actuarial science can help clarify goals and build motivation for the journey ahead."
  },
  {
    "objectID": "posts/actuary-in-nepal.html#how-to-find-internships-in-nepal",
    "href": "posts/actuary-in-nepal.html#how-to-find-internships-in-nepal",
    "title": "How to Become an Actuary in Nepal",
    "section": "How to Find Internships in Nepal",
    "text": "How to Find Internships in Nepal\nThe best place to start your search for actuarial internships is with insurance companies, both life and non-life, as well as consulting firms that offer actuarial services. Many of these organizations are based in Kathmandu, but opportunities are expanding as the industry grows. Begin by researching the major players in the market—such as Nepal Life Insurance, National Life Insurance, and Shikhar Insurance—and visit their websites for career or internship announcements.\nNetworking is also essential. Attend events organized by the Actuarial Society of Nepal, university career fairs, and industry seminars. Don’t hesitate to reach out directly to HR departments or actuarial teams with a polite email expressing your interest and attaching your CV. LinkedIn is a valuable tool for connecting with professionals and learning about openings. If you are a student at Tribhuvan University or another institution, ask your professors or department for leads, as they often have industry contacts."
  },
  {
    "objectID": "posts/actuary-in-nepal.html#sample-cvs-and-cover-letters",
    "href": "posts/actuary-in-nepal.html#sample-cvs-and-cover-letters",
    "title": "How to Become an Actuary in Nepal",
    "section": "Sample CVs and Cover Letters",
    "text": "Sample CVs and Cover Letters\nA strong CV and cover letter are your first chance to make a good impression. Your CV should highlight your academic achievements, relevant coursework, technical skills (such as proficiency in R, Python, or Excel), and any extracurricular activities that demonstrate leadership or teamwork. If you have passed any actuarial exams or completed relevant projects, be sure to include them.\nSample CV Outline: - Name and contact information - Education (degree, university, expected graduation date) - Relevant coursework (probability, statistics, financial mathematics, etc.) - Technical skills (programming languages, software) - Actuarial exams passed (if any) - Projects or research experience - Extracurricular activities and leadership roles - References (optional)\nYour cover letter should be concise and tailored to the specific organization. Explain why you are interested in actuarial science, what you hope to learn from the internship, and how your skills and experiences make you a good fit. Show enthusiasm and a willingness to contribute."
  },
  {
    "objectID": "posts/actuary-in-nepal.html#interview-tips",
    "href": "posts/actuary-in-nepal.html#interview-tips",
    "title": "How to Become an Actuary in Nepal",
    "section": "Interview Tips",
    "text": "Interview Tips\nIf you are invited for an interview, preparation is key. Review the basics of actuarial science, be ready to discuss your coursework and projects, and practice explaining complex concepts in simple terms. Employers may ask technical questions (e.g., about probability or financial mathematics) as well as behavioral questions to assess your teamwork, problem-solving, and communication skills.\nDress professionally, arrive on time, and bring copies of your CV. Be honest about your experience—if you don’t know the answer to a technical question, explain how you would approach solving it. Show curiosity and a willingness to learn. After the interview, send a thank-you email to express your appreciation for the opportunity."
  },
  {
    "objectID": "posts/actuary-in-nepal.html#soft-skills-for-actuaries",
    "href": "posts/actuary-in-nepal.html#soft-skills-for-actuaries",
    "title": "How to Become an Actuary in Nepal",
    "section": "Soft Skills for Actuaries",
    "text": "Soft Skills for Actuaries\nWhile technical expertise forms the foundation of actuarial work, soft skills are what set outstanding actuaries apart. The ability to communicate complex ideas clearly—whether in written reports or verbal presentations—is crucial, especially when working with colleagues from non-technical backgrounds. Actuaries must often explain the implications of their analyses to business leaders, regulators, or clients who may not be familiar with statistical models or financial mathematics. Clarity, patience, and empathy are invaluable in these situations.\nTeamwork is another vital skill. Actuaries rarely work in isolation; they collaborate with underwriters, product managers, IT specialists, and executives. Being able to listen actively, respect diverse perspectives, and contribute constructively to group discussions enhances both the quality of work and the workplace environment. Leadership skills, such as taking initiative, mentoring junior colleagues, and managing projects, become increasingly important as you advance in your career.\nAdaptability and a willingness to learn are essential in a rapidly changing field. The insurance and finance industries are constantly evolving, with new regulations, technologies, and risks emerging all the time. Successful actuaries embrace change, seek out new knowledge, and are open to feedback and self-improvement."
  },
  {
    "objectID": "posts/actuary-in-nepal.html#networking-conferences-and-workshops",
    "href": "posts/actuary-in-nepal.html#networking-conferences-and-workshops",
    "title": "How to Become an Actuary in Nepal",
    "section": "Networking, Conferences, and Workshops",
    "text": "Networking, Conferences, and Workshops\nBuilding a professional network is one of the most effective ways to advance your actuarial career. In Nepal, the actuarial community is still small but growing, which makes networking both accessible and impactful. Start by joining the Actuarial Society of Nepal (ASN), which organizes regular seminars, workshops, and social events. These gatherings provide opportunities to meet experienced actuaries, learn about industry trends, and discover job or internship openings.\nAttending conferences—whether local, regional, or international—broadens your perspective and exposes you to the latest research and best practices. Many actuarial societies, such as the IFoA, SOA, and IAI, offer student memberships that grant access to webinars, online forums, and global events. Participating in these activities not only enhances your knowledge but also helps you build relationships with peers and mentors who can support your professional growth.\nDon’t underestimate the power of informal networking. Connect with classmates, professors, and colleagues on platforms like LinkedIn. Engage in online actuarial communities, contribute to discussions, and share your experiences. Over time, your network will become a valuable source of advice, encouragement, and opportunity."
  },
  {
    "objectID": "posts/actuary-in-nepal.html#continuing-education",
    "href": "posts/actuary-in-nepal.html#continuing-education",
    "title": "How to Become an Actuary in Nepal",
    "section": "Continuing Education",
    "text": "Continuing Education\nThe actuarial profession is defined by a commitment to lifelong learning. After qualifying as an actuary, you are expected to maintain and expand your knowledge through continuing professional development (CPD). This may include attending workshops, completing online courses, reading industry publications, or participating in research projects.\nIn Nepal, as the industry matures, there is increasing emphasis on CPD. The Actuarial Society of Nepal and other professional bodies regularly offer training sessions on emerging topics such as data analytics, enterprise risk management, and regulatory changes. International societies also provide extensive resources, including e-learning modules, technical papers, and case studies.\nStaying current is not just a regulatory requirement—it is essential for remaining relevant and effective in your role. Make it a habit to set learning goals each year, seek feedback from peers and supervisors, and explore new areas of interest. The most successful actuaries are those who never stop learning."
  },
  {
    "objectID": "posts/actuary-in-nepal.html#official-actuarial-societies",
    "href": "posts/actuary-in-nepal.html#official-actuarial-societies",
    "title": "How to Become an Actuary in Nepal",
    "section": "Official Actuarial Societies",
    "text": "Official Actuarial Societies\n\nActuarial Society of Nepal (ASN): asn.org.np — The main professional body for actuaries in Nepal. Offers events, networking, and local guidance.\nNepal Insurance Authority (NIA): nia.gov.np — Regulatory authority for insurance and actuarial practice in Nepal.\nInstitute and Faculty of Actuaries (IFoA): actuaries.org.uk — UK-based society with global recognition, exam information, and resources.\nSociety of Actuaries (SOA): soa.org — US-based society, especially strong in life and health insurance, with extensive study resources.\nInstitute of Actuaries of India (IAI): actuariesindia.org — Indian society, popular among Nepali students.\nCasualty Actuarial Society (CAS): casact.org — US-based, focused on property and casualty insurance."
  },
  {
    "objectID": "posts/actuary-in-nepal.html#additional-resources",
    "href": "posts/actuary-in-nepal.html#additional-resources",
    "title": "How to Become an Actuary in Nepal",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nLinkedIn: Build your professional profile and connect with actuaries in Nepal and abroad.\nYouTube Channels: Search for actuarial exam walkthroughs, career talks, and technical tutorials.\nOpen Access Journals: Explore research in insurance, risk, and actuarial science for deeper learning.\n\nBookmark these resources and revisit them regularly. The actuarial journey is challenging, but with the right support and information, you can navigate it successfully and make a meaningful impact in Nepal’s growing financial sector."
  },
  {
    "objectID": "posts/ml-Decoding Personality.html#what-is-a-decision-tree",
    "href": "posts/ml-Decoding Personality.html#what-is-a-decision-tree",
    "title": "Decision Trees: A Complete Guide for Beginners",
    "section": "",
    "text": "A decision tree is a flowchart-like structure used to make decisions or predictions. Each internal node of the tree represents a test or question about a feature (for example, “Is age &gt; 30?”), each branch represents the outcome of the test, and each leaf node represents a final decision or prediction. Decision trees can be used for both classification (predicting categories) and regression (predicting numbers).\nImagine you want to decide whether to play tennis based on the weather. A decision tree might first ask, “Is it sunny?” If yes, it might then ask, “Is the humidity high?” and so on, until it reaches a decision like “Play” or “Don’t play.”"
  },
  {
    "objectID": "posts/ml-Decoding Personality.html#key-terms-in-decision-trees",
    "href": "posts/ml-Decoding Personality.html#key-terms-in-decision-trees",
    "title": "Decision Trees: A Complete Guide for Beginners",
    "section": "",
    "text": "Before we dive deeper, let’s define some important terms:\n\nRoot Node: The top node of the tree, where the first split or question is made.\nInternal Node: Any node that splits into further branches (not a leaf).\nLeaf Node (Terminal Node): The end node that gives the final output (class or value).\nBranch: A path from one node to another, representing the outcome of a test.\nSplit: The process of dividing a node into two or more sub-nodes based on a feature.\nFeature (Attribute): A variable or column in your dataset used to split the data.\nDepth: The number of levels in the tree from the root to the deepest leaf."
  },
  {
    "objectID": "posts/ml-Decoding Personality.html#types-of-decision-trees-classification-vs.-regression",
    "href": "posts/ml-Decoding Personality.html#types-of-decision-trees-classification-vs.-regression",
    "title": "Decision Trees: A Complete Guide for Beginners",
    "section": "",
    "text": "Decision trees are divided into two main types, depending on the nature of the output variable:\n\n\nClassification trees are used when the target variable is categorical—that is, when you want to predict a class or label (such as “spam” vs. “not spam,” or “disease” vs. “no disease”). At each node, the tree asks a question that splits the data into groups that are more homogeneous with respect to the target class.\nExample: Suppose you want to predict whether a loan applicant will default (“Yes” or “No”). The tree might split on features like income, credit score, or employment status, eventually leading to a prediction at the leaf node.\n\n\nTo decide the best way to split the data at each node, classification trees use metrics that measure how “pure” or homogeneous the resulting groups are. The most common metrics are:\n\nGini Impurity: Measures how often a randomly chosen element would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the node. Lower Gini means purer nodes.\nEntropy (Information Gain): Measures the amount of disorder or uncertainty. Splits that reduce entropy the most are preferred.\n\nHow to choose splits: At each node, the algorithm tries all possible splits and chooses the one that results in the greatest reduction in impurity (Gini or Entropy).\nEvaluation Metrics: After building the tree, we evaluate its performance using metrics such as: - Accuracy: The proportion of correct predictions. - Precision, Recall, F1 Score: Useful for imbalanced datasets. - Confusion Matrix: Shows the counts of true positives, false positives, etc.\n\n\n\n\nRegression trees are used when the target variable is continuous or numerical (such as predicting house prices or temperatures). Instead of predicting a class, the tree predicts a number.\nExample: Suppose you want to predict the price of a house based on features like size, location, and number of bedrooms. The regression tree splits the data at each node to minimize the difference between the predicted and actual values.\n\n\nTo choose the best splits, regression trees use metrics that measure how well the split reduces the variability of the target variable. The most common metrics are:\n\nMean Squared Error (MSE): The average of the squared differences between predicted and actual values.\nMean Absolute Error (MAE): The average of the absolute differences between predicted and actual values.\n\nHow to choose splits: At each node, the algorithm tries all possible splits and chooses the one that results in the greatest reduction in error (MSE or MAE).\nEvaluation Metrics: After building the tree, we evaluate its performance using metrics such as: - R-squared (R²): Measures how well the model explains the variability of the target. - Root Mean Squared Error (RMSE): The square root of MSE, in the same units as the target."
  },
  {
    "objectID": "posts/ml-Decoding Personality.html#advantages-and-limitations-of-decision-trees",
    "href": "posts/ml-Decoding Personality.html#advantages-and-limitations-of-decision-trees",
    "title": "Decision Trees: A Complete Guide for Beginners",
    "section": "",
    "text": "Advantages: - Easy to understand and interpret. - Can handle both numerical and categorical data. - Require little data preparation. - Can model non-linear relationships.\nLimitations: - Prone to overfitting (creating trees that are too complex and fit the training data too closely). - Can be unstable—small changes in data can lead to different trees. - Less accurate than some other algorithms (like random forests or boosting) on complex problems."
  },
  {
    "objectID": "posts/ml-Decoding Personality.html#step-by-step-example-building-a-decision-tree-to-predict-personality-type",
    "href": "posts/ml-Decoding Personality.html#step-by-step-example-building-a-decision-tree-to-predict-personality-type",
    "title": "Decision Trees: A Complete Guide for Beginners",
    "section": "",
    "text": "Let’s walk through a practical example using R, where we predict whether a person is an introvert or extrovert using a decision tree. We’ll cover every step: reading the data, cleaning it, splitting into training and test sets, building the tree, evaluating it, and pruning for better performance.\n\n\nFirst, we load the necessary libraries and read the dataset directly from the provided URL. We’ll use Quarto chunk options to suppress warnings and messages for a cleaner output.\n\nlibrary(readr)\nlibrary(janitor)\nlibrary(dplyr)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(caret) # for train/test split\n\n# Read the data\ndf &lt;- read_csv(\"https://raw.githubusercontent.com/IKSHRESTHA/Actuarial-Reflections/refs/heads/main/data/06272925/personality_datasert.csv\") |&gt; \n  janitor::clean_names()\n\n# Inspect the data\nstr(df)\n\nspc_tbl_ [2,900 × 8] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ time_spent_alone         : num [1:2900] 4 9 9 0 3 1 4 2 10 0 ...\n $ stage_fear               : chr [1:2900] \"No\" \"Yes\" \"Yes\" \"No\" ...\n $ social_event_attendance  : num [1:2900] 4 0 1 6 9 7 9 8 1 8 ...\n $ going_outside            : num [1:2900] 6 0 2 7 4 5 3 4 3 6 ...\n $ drained_after_socializing: chr [1:2900] \"No\" \"Yes\" \"Yes\" \"No\" ...\n $ friends_circle_size      : num [1:2900] 13 0 5 14 8 6 7 7 0 13 ...\n $ post_frequency           : num [1:2900] 5 3 2 8 5 6 7 8 3 8 ...\n $ personality              : chr [1:2900] \"Extrovert\" \"Introvert\" \"Introvert\" \"Extrovert\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Time_spent_Alone = col_double(),\n  ..   Stage_fear = col_character(),\n  ..   Social_event_attendance = col_double(),\n  ..   Going_outside = col_double(),\n  ..   Drained_after_socializing = col_character(),\n  ..   Friends_circle_size = col_double(),\n  ..   Post_frequency = col_double(),\n  ..   Personality = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nsummary(df)\n\n time_spent_alone  stage_fear        social_event_attendance going_outside\n Min.   : 0.000   Length:2900        Min.   : 0.000          Min.   :0    \n 1st Qu.: 2.000   Class :character   1st Qu.: 2.000          1st Qu.:1    \n Median : 4.000   Mode  :character   Median : 3.963          Median :3    \n Mean   : 4.506                      Mean   : 3.963          Mean   :3    \n 3rd Qu.: 7.000                      3rd Qu.: 6.000          3rd Qu.:5    \n Max.   :11.000                      Max.   :10.000          Max.   :7    \n drained_after_socializing friends_circle_size post_frequency  \n Length:2900               Min.   : 0.000      Min.   : 0.000  \n Class :character          1st Qu.: 3.000      1st Qu.: 1.000  \n Mode  :character          Median : 5.000      Median : 3.000  \n                           Mean   : 6.269      Mean   : 3.565  \n                           3rd Qu.:10.000      3rd Qu.: 6.000  \n                           Max.   :15.000      Max.   :10.000  \n personality       \n Length:2900       \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n\nThe output above shows the structure and summary statistics of the dataset. You can see the variable types, ranges, and a quick overview of the data. This helps us understand what features are available and if there are any obvious data quality issues.\n\n\n\nWe’ll make sure the target variable (personality) is a factor, and check for missing values. We’ll also suppress warnings and messages in this chunk.\n\n# Convert target to factor\ndf$personality &lt;- as.factor(df$personality)\n\n# Check for missing values\ncolSums(is.na(df))\n\n         time_spent_alone                stage_fear   social_event_attendance \n                        0                         0                         0 \n            going_outside drained_after_socializing       friends_circle_size \n                        0                         0                         0 \n           post_frequency               personality \n                        0                         0 \n\n\nThe output will show the number of missing values in each column. If all values are zero, there are no missing data to worry about. If not, you may need to handle them before modeling.\n\n\n\nWe’ll split the data into 70% training and 30% testing sets to evaluate our model’s performance on unseen data.\n\nset.seed(123) # for reproducibility\ntrain_index &lt;- createDataPartition(df$personality, p = 0.7, list = FALSE)\ntrain_data &lt;- df[train_index, ]\ntest_data &lt;- df[-train_index, ]\n\nThis step ensures that our model is trained on one portion of the data and tested on another, helping us assess how well it generalizes to new cases.\n\n\n\nNow, we’ll build a classification tree to predict personality using all other variables.\n\ntree_model &lt;- rpart(personality ~ ., data = train_data, method = \"class\", cp = 0.01)\n\n# Visualize the tree\nrpart.plot(tree_model)\n\n\n\n\n\n\n\n\nThe plot above shows the structure of the decision tree. Each node represents a split based on a feature, and the leaves show the predicted class (introvert or extrovert).\n\n\n\nWe’ll use the test set to see how well our tree predicts introverts vs. extroverts.\n\npred &lt;- predict(tree_model, test_data, type = \"class\")\nconfusionMatrix(pred, test_data$personality)\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  Extrovert Introvert\n  Extrovert       412        29\n  Introvert        35       393\n                                          \n               Accuracy : 0.9264          \n                 95% CI : (0.9069, 0.9428)\n    No Information Rate : 0.5144          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.8526          \n                                          \n Mcnemar's Test P-Value : 0.532           \n                                          \n            Sensitivity : 0.9217          \n            Specificity : 0.9313          \n         Pos Pred Value : 0.9342          \n         Neg Pred Value : 0.9182          \n             Prevalence : 0.5144          \n         Detection Rate : 0.4741          \n   Detection Prevalence : 0.5075          \n      Balanced Accuracy : 0.9265          \n                                          \n       'Positive' Class : Extrovert       \n                                          \n\n\nThe confusion matrix output will display the number of correct and incorrect predictions for each class. Accuracy, sensitivity, and specificity are also shown, helping you judge the model’s performance.\n\n\n\nDecision trees can overfit, so pruning helps simplify the tree and improve generalization. We’ll use the complexity parameter (cp) to prune.\n\n# Find optimal cp value\nprintcp(tree_model)\n\n\nClassification tree:\nrpart(formula = personality ~ ., data = train_data, method = \"class\", \n    cp = 0.01)\n\nVariables actually used in tree construction:\n[1] drained_after_socializing stage_fear               \n\nRoot node error: 987/2031 = 0.48597\n\nn= 2031 \n\n        CP nsplit rel error  xerror     xstd\n1 0.855117      0   1.00000 1.00000 0.022821\n2 0.016211      1   0.14488 0.14488 0.011681\n3 0.010000      2   0.12867 0.12969 0.011096\n\n# Choose the cp with lowest cross-validated error\nbest_cp &lt;- tree_model$cptable[which.min(tree_model$cptable[,\"xerror\"]), \"CP\"]\n\n# Prune the tree\npruned_tree &lt;- prune(tree_model, cp = best_cp)\n\n# Visualize pruned tree\nrpart.plot(pruned_tree)\n\n\n\n\n\n\n\n# Evaluate pruned tree\npruned_pred &lt;- predict(pruned_tree, test_data, type = \"class\")\nconfusionMatrix(pruned_pred, test_data$personality)\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  Extrovert Introvert\n  Extrovert       412        29\n  Introvert        35       393\n                                          \n               Accuracy : 0.9264          \n                 95% CI : (0.9069, 0.9428)\n    No Information Rate : 0.5144          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.8526          \n                                          \n Mcnemar's Test P-Value : 0.532           \n                                          \n            Sensitivity : 0.9217          \n            Specificity : 0.9313          \n         Pos Pred Value : 0.9342          \n         Neg Pred Value : 0.9182          \n             Prevalence : 0.5144          \n         Detection Rate : 0.4741          \n   Detection Prevalence : 0.5075          \n      Balanced Accuracy : 0.9265          \n                                          \n       'Positive' Class : Extrovert       \n                                          \n\n\nAfter pruning, the tree is simpler and less likely to overfit. The new confusion matrix shows how well the pruned tree performs on the test data. Compare this to the previous results to see if pruning improved generalization.\n\n\n\n\nWe loaded and cleaned the data (with warnings suppressed for clarity).\nSplit it into training and test sets.\nBuilt and visualized a decision tree to predict personality type.\nEvaluated its performance with a confusion matrix.\nPruned the tree and compared results.\n\nThis step-by-step approach helps you understand not just how to build a decision tree, but also how to interpret the output and ensure it performs well on new, unseen data."
  },
  {
    "objectID": "posts/understanding risk.html#what-is-risk-a-deeper-look",
    "href": "posts/understanding risk.html#what-is-risk-a-deeper-look",
    "title": "Understanding Risk",
    "section": "",
    "text": "To truly understand risk, it helps to break it down into its core elements. At its heart, risk is defined by two main factors: probability and impact. Probability is the likelihood that a particular event will occur, while impact is the severity of the consequences if it does. For example, the risk of rain on a given day depends on the weather forecast (probability), but the impact of that rain will be very different if you are planning a picnic versus if you are a farmer hoping for crops to grow.\nIn business and insurance, risk is often described as the combination of the chance that something will go wrong and the cost or harm that would result. Managing risk means understanding both how often something might go wrong and how serious it would be if it does. This dual perspective allows individuals and organizations to make informed decisions about which risks to accept, which to avoid, and which to transfer to others (such as through insurance)."
  },
  {
    "objectID": "posts/understanding risk.html#everyday-examples-of-risk",
    "href": "posts/understanding risk.html#everyday-examples-of-risk",
    "title": "Understanding Risk",
    "section": "",
    "text": "Risk is everywhere. When you drive a car, you face the risk of an accident. When you invest money, you risk losing it if the market falls. Even something as simple as eating at a new restaurant carries the risk of food poisoning. Some risks are so minor that we barely notice them, while others can have life-changing consequences.\nConsider a small business owner. She faces the risk that her products might not sell, that a fire could damage her shop, or that an employee might get injured at work. Each of these risks has a different probability and impact, and each requires a different approach to management."
  },
  {
    "objectID": "posts/understanding risk.html#the-elements-of-risk-probability-and-impact",
    "href": "posts/understanding risk.html#the-elements-of-risk-probability-and-impact",
    "title": "Understanding Risk",
    "section": "",
    "text": "When thinking about risk, always ask two questions:\n\nHow likely is it to happen? (Probability)\nHow bad would it be if it happens? (Impact)\n\nFor example, the risk of a minor power outage in a city might be fairly high (it happens often), but the impact is usually small (a brief inconvenience). In contrast, the risk of a major earthquake is low (it rarely happens), but the impact can be catastrophic."
  },
  {
    "objectID": "posts/understanding risk.html#types-of-risk-a-comprehensive-guide",
    "href": "posts/understanding risk.html#types-of-risk-a-comprehensive-guide",
    "title": "Understanding Risk",
    "section": "",
    "text": "Risks can be grouped in many ways, but one of the most useful is by considering both their likelihood and their impact. This approach helps prioritize which risks deserve the most attention. Let’s explore four common types of risk, with detailed examples and management strategies for each.\n\n\nMinor risks are those that are unlikely to happen and, even if they do, would not cause much harm. These are the everyday annoyances and small setbacks that are part of life. For example, running out of printer paper at the office is a minor risk. It might slow you down for a few minutes, but it is easily fixed and rarely has serious consequences.\nIn most cases, the best way to manage minor risks is simply to accept them. It is not worth spending a lot of time or money trying to prevent every small inconvenience. However, it is wise to monitor these risks to make sure they do not become more serious over time. For instance, if running out of printer paper starts happening every week, it might be a sign that you need a better system for ordering supplies.\n\n\n\nSome risks are unlikely to occur, but if they do, the consequences can be severe. These are the risks that keep business owners and families awake at night. Natural disasters like earthquakes, floods, or fires fall into this category. The probability of a major earthquake in a given year is low, but the impact can be devastating—destroying homes, businesses, and lives.\nManaging rare but serious risks requires careful planning. One common strategy is to develop contingency plans—detailed steps to follow if the worst happens. For example, a family might have an emergency kit and a plan for where to meet if their home is damaged in an earthquake. Businesses often buy insurance to transfer some of the financial risk to an insurer. Regularly reviewing and updating these plans is essential, as circumstances and risks can change over time.\n\n\n\nThese are risks that happen often, but their effects are minor. For example, a company might experience frequent minor IT glitches that slow down work but do not cause major losses. In a household, this could be the risk of small kitchen accidents, like spilling water or burning toast.\nThe best way to manage frequent, low-impact risks is to reduce how often they happen. This might mean improving processes, providing better training, or maintaining equipment more regularly. Keeping records of how often these risks occur can help spot trends and identify areas for improvement. For example, if a company notices that IT glitches are becoming more common, it might be time to upgrade its systems or provide additional staff training.\n\n\n\nCritical risks are both likely to happen and would have major consequences. These are the risks that demand immediate attention and strong controls. For example, a hospital faces the critical risk of a power failure during surgery. The probability may not be high, but the impact is so severe that it cannot be ignored. Another example is the risk of a cyberattack on a company that stores sensitive customer data. Such an event is both increasingly likely and potentially disastrous.\nManaging critical risks requires a proactive approach. Organizations must act immediately to address these risks, implementing strong controls and safeguards. This might include installing backup generators in a hospital, setting up firewalls and security protocols for IT systems, or developing and testing detailed response plans. Regular monitoring and review are essential to ensure that controls remain effective as threats evolve."
  },
  {
    "objectID": "posts/understanding risk.html#insurable-vs.-uninsurable-risks",
    "href": "posts/understanding risk.html#insurable-vs.-uninsurable-risks",
    "title": "Understanding Risk",
    "section": "",
    "text": "One of the most important questions in actuarial science and insurance is whether a risk is insurable. Not all risks can be covered by insurance, and understanding the difference is crucial for both individuals and businesses.\n\n\nFor a risk to be insurable, it generally needs to meet several criteria:\n\nThe risk must be definable and measurable. Insurers need to know what event they are covering and be able to estimate the probability and potential loss.\nThe loss must be accidental and unintentional. Insurance is designed to cover unforeseen events, not losses that are certain or deliberate.\nThe loss must be significant enough to cause financial hardship, but not so catastrophic that it would bankrupt the insurer.\nThere must be a large number of similar exposure units. This allows insurers to pool risks and use the law of large numbers to predict losses.\nThe probability of loss must be calculable. Insurers rely on data and statistics to set premiums and reserves.\nThe premium must be affordable. If the cost of insurance is too high, few people will buy it.\n\n\n\n\nMost common insurance policies cover risks that meet these criteria. For example:\n\nFire insurance covers the risk of accidental fire damaging a home or business. Fires are relatively rare, but the losses can be significant, and insurers have enough data to estimate the probability and cost.\nHealth insurance covers the risk of illness or injury. While everyone gets sick at some point, the timing and severity are unpredictable, and insurers can pool risks across many policyholders.\nAuto insurance covers the risk of car accidents. Again, accidents are accidental, measurable, and there is enough data to set premiums.\n\n\n\n\nSome risks cannot be insured, either because they are too certain, too catastrophic, or impossible to measure. For example:\n\nWear and tear on a car or machine is not insurable, because it is certain to happen over time.\nLosses from illegal activities or intentional acts are not covered, because insurance is not meant to reward bad behavior.\nWar and nuclear disasters are often excluded from insurance policies, because the potential losses are so large and unpredictable that no insurer could cover them.\nSpeculative risks, such as gambling losses or investment losses, are generally not insurable, because they involve the chance of gain as well as loss, and are not accidental."
  },
  {
    "objectID": "posts/understanding risk.html#real-world-scenarios-insurable-and-uninsurable-risks",
    "href": "posts/understanding risk.html#real-world-scenarios-insurable-and-uninsurable-risks",
    "title": "Understanding Risk",
    "section": "",
    "text": "Let’s look at some practical examples to make these concepts clearer:\nScenario 1: Insurable Risk\nSuppose you own a small bakery. You are worried about the risk of a fire destroying your shop. This is an insurable risk: it is accidental, measurable, and there is enough data for insurers to set a fair premium. You can buy fire insurance to protect your business.\nScenario 2: Uninsurable Risk\nNow imagine you are concerned that your bakery might not be popular and could fail because customers do not like your bread. This is a business risk, but it is not insurable. The risk of business failure due to lack of demand is too uncertain, and it is influenced by your own actions and market conditions. No insurer will cover this type of risk.\nScenario 3: Partially Insurable Risk\nSuppose you are a farmer worried about drought. Some types of crop insurance exist, but not all weather risks are insurable everywhere. If drought is a common, measurable risk in your area, insurers may offer coverage. But if the risk is too frequent or severe, or if there is not enough data, it may be uninsurable or only partially covered."
  },
  {
    "objectID": "posts/understanding risk.html#why-understanding-risk-matters",
    "href": "posts/understanding risk.html#why-understanding-risk-matters",
    "title": "Understanding Risk",
    "section": "",
    "text": "Understanding risk—and knowing which risks are insurable—is essential for making smart decisions in life and business. It helps you focus your efforts and resources on the risks that matter most, and it allows you to use insurance and other tools effectively to protect yourself from financial loss.\nGood risk management means identifying your most important risks, understanding their likelihood and impact, and taking appropriate action. Sometimes that means accepting small risks, sometimes it means preparing for rare disasters, and sometimes it means transferring risk to an insurer. The key is to be proactive, informed, and realistic about what you can and cannot control."
  },
  {
    "objectID": "posts/understanding risk.html#summary-table-types-of-risk-and-management-strategies",
    "href": "posts/understanding risk.html#summary-table-types-of-risk-and-management-strategies",
    "title": "Understanding Risk",
    "section": "",
    "text": "Type of Risk\nExample\nHow to Manage\nInsurable?\n\n\n\n\nMinor\nPrinter out of paper\nAccept, Monitor\nNo\n\n\nRare but Serious\nEarthquake\nPlan, Transfer (insurance), Monitor\nSometimes (if data exists)\n\n\nFrequent, Low-Impact\nSmall IT glitches\nReduce, Monitor\nNo\n\n\nCritical\nCyberattack/data breach\nAct, Control, Respond, Monitor\nYes (with limits)\n\n\nFire\nFire in a bakery\nPrevent, Insure, Respond\nYes\n\n\nBusiness Failure\nBakery not popular\nBusiness planning, Diversify\nNo\n\n\nWear and Tear\nCar engine wears out\nMaintenance\nNo\n\n\nWar/Nuclear Disaster\nWar damages property\nGovernment aid (rare), Not insurable\nNo\n\n\n\n\nIn conclusion, risk is a complex but manageable part of life. By understanding its elements, types, and insurability, you can make better decisions, protect yourself and your business, and focus your energy where it matters most. Actuaries and insurers play a vital role in helping society manage risk, but everyone benefits from a deeper understanding of how risk works and how to respond to it."
  }
]